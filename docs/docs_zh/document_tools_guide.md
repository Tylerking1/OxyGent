# æ–‡æ¡£å¤„ç†å·¥å…·é›†ä½¿ç”¨æŒ‡å—

> OxyGent æ–‡æ¡£å¤„ç†å·¥å…·é›† - æ”¯æŒ PDFã€Wordã€Excel ç­‰å¤šç§æ–‡æ¡£æ ¼å¼çš„æ™ºèƒ½å¤„ç†

## ğŸ“š ç›®å½•

- [ç®€ä»‹](#ç®€ä»‹)
- [ç‰¹æ€§äº®ç‚¹](#ç‰¹æ€§äº®ç‚¹)
- [å®‰è£…ä¾èµ–](#å®‰è£…ä¾èµ–)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å·¥å…·APIè¯¦è§£](#å·¥å…·apiè¯¦è§£)
- [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)

---

## ç®€ä»‹

æ–‡æ¡£å¤„ç†å·¥å…·é›†æ˜¯ OxyGent å†…ç½®çš„æ–‡æ¡£å¤„ç†æ¨¡å—ï¼Œæä¾›å¯¹ä¸»æµæ–‡æ¡£æ ¼å¼çš„å…¨é¢æ”¯æŒã€‚æ— è®ºæ˜¯ç®€å•çš„æ–‡æœ¬æå–ï¼Œè¿˜æ˜¯å¤æ‚çš„æ–‡æ¡£åˆ†æï¼Œéƒ½èƒ½è½»æ¾å®Œæˆã€‚

### æ”¯æŒæ ¼å¼

| æ ¼å¼ | è¯»å– | å†™å…¥ | è¡¨æ ¼æå– | å›¾åƒæå– | å…ƒæ•°æ® |
|------|------|------|----------|----------|--------|
| **PDF** | âœ… | âŒ | âœ… | âœ… | âœ… |
| **Word (.docx)** | âœ… | âŒ* | âœ… | âŒ | âŒ |
| **Excel (.xlsx)** | âœ… | âŒ* | N/A | âŒ | âœ… |
| **PowerPoint (.pptx)** | ğŸ”„ | âŒ | âŒ | âŒ | âŒ |

*æ ‡æ³¨ï¼šå†™å…¥åŠŸèƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­æä¾›  
ğŸ”„ = å¼€å‘ä¸­

---

## ç‰¹æ€§äº®ç‚¹

### ğŸš€ é«˜æ€§èƒ½
- **æ¯”PyPDF2å¿«10-50å€**ï¼šé‡‡ç”¨PyMuPDFçš„C++åº•å±‚å¼•æ“
- **å†…å­˜ä¼˜åŒ–**ï¼šæµå¼å¤„ç†å¤§æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
- **å¹¶å‘æ”¯æŒ**ï¼šæ”¯æŒæ‰¹é‡æ–‡æ¡£çš„å¹¶è¡Œå¤„ç†

### ğŸ¯ é«˜ç²¾åº¦
- **è¡¨æ ¼è¯†åˆ«ç‡90%+**ï¼šä½¿ç”¨pdfplumberçš„é«˜çº§ç®—æ³•
- **æ™ºèƒ½å¸ƒå±€åˆ†æ**ï¼šå‡†ç¡®è¯†åˆ«æ–‡æ¡£ç»“æ„
- **ä¸­è‹±æ–‡å…¨æ”¯æŒ**ï¼šå®Œç¾å¤„ç†å¤šè¯­è¨€æ–‡æ¡£

### ğŸ”§ æ˜“ç”¨æ€§
- **ç»Ÿä¸€APIè®¾è®¡**ï¼šæ‰€æœ‰å·¥å…·éµå¾ªç›¸åŒçš„è°ƒç”¨æ¨¡å¼
- **è¯¦ç»†é”™è¯¯æç¤º**ï¼šæ¸…æ™°çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³å»ºè®®
- **JSONæ ¼å¼è¿”å›**ï¼šç»“æ„åŒ–æ•°æ®ï¼Œæ˜“äºç¨‹åºå¤„ç†

### ğŸ¤– AIé›†æˆ
- **åŸç”ŸAgentæ”¯æŒ**ï¼šå¯ç›´æ¥æ³¨å†Œä¸ºAgentå·¥å…·
- **æ™ºèƒ½æ–‡æ¡£åˆ†æ**ï¼šç»“åˆLLMå®ç°æ·±åº¦å†…å®¹ç†è§£
- **RAGä¼˜åŒ–**ï¼šä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆåœºæ™¯ç‰¹åˆ«ä¼˜åŒ–

---

## å®‰è£…ä¾èµ–

### å®Œæ•´å®‰è£…ï¼ˆæ¨èï¼‰

```bash
# å®‰è£…æ‰€æœ‰æ–‡æ¡£å¤„ç†ä¾èµ–
pip install PyMuPDF pdfplumber python-docx openpyxl
```

### æŒ‰éœ€å®‰è£…

```bash
# ä»…å¤„ç†PDF
pip install PyMuPDF pdfplumber

# ä»…å¤„ç†Word
pip install python-docx

# ä»…å¤„ç†Excel
pip install openpyxl
```


### ä¾èµ–è¯´æ˜

| åº“åç§° | ç‰ˆæœ¬è¦æ±‚ | ç”¨é€” | è®¸å¯è¯ |
|--------|----------|------|--------|
| PyMuPDF | â‰¥1.23.0 | PDFæ ¸å¿ƒå¤„ç† | AGPL/å•†ä¸šåŒè®¸å¯ |
| pdfplumber | â‰¥0.10.0 | è¡¨æ ¼ç²¾å‡†æå– | MIT |
| python-docx | â‰¥1.1.0 | Wordæ–‡æ¡£å¤„ç† | MIT |
| openpyxl | â‰¥3.1.0 | Excelæ–‡æ¡£å¤„ç† | MIT |

---

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼1ï¼šç›´æ¥è°ƒç”¨å·¥å…·API

æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ï¼Œé€‚åˆè„šæœ¬åŒ–å¤„ç†ï¼š

```python
from oxygent.preset_tools.document_tools import extract_pdf_text
import json

# æå–PDFæ–‡æœ¬
result = extract_pdf_text("report.pdf", page_range="1-5")
data = json.loads(result)

if data['success']:
    for page in data['pages']:
        print(f"ç¬¬{page['page_number']}é¡µ:")
        print(page['text'][:200])  # æ‰“å°å‰200å­—ç¬¦
```

### æ–¹å¼2ï¼šä½¿ç”¨Agentæ™ºèƒ½å¤„ç†

è®©AIç†è§£æ–‡æ¡£å¤„ç†éœ€æ±‚ï¼š

```python
import asyncio
from oxygent import MAS, Config, oxy, preset_tools

Config.set_agent_llm_model("default_llm")

oxy_space = [
    oxy.HttpLLM(
        name="default_llm",
        api_key="your_api_key",
        base_url="your_base_url",
        model_name="your_model",
    ),
    preset_tools.document_tools,  # æ³¨å†Œæ–‡æ¡£å·¥å…·
    oxy.ReActAgent(
        name="doc_agent",
        desc="æ–‡æ¡£å¤„ç†åŠ©æ‰‹",
        tools=["document_tools"],
    ),
]

async def main():
    async with MAS(oxy_space=oxy_space) as mas:
        await mas.start_web_service(
            first_query="æå–è¿™ä¸ªPDFçš„æ‰€æœ‰è¡¨æ ¼æ•°æ®ï¼šreport.pdf"
        )

asyncio.run(main())
```

### æ–¹å¼3ï¼šé›†æˆåˆ°ç°æœ‰Agent

```python
# ä¸ºç°æœ‰Agentæ·»åŠ æ–‡æ¡£å¤„ç†èƒ½åŠ›
oxy_space = [
    your_llm,
    preset_tools.document_tools,  # æ·»åŠ æ–‡æ¡£å·¥å…·
    preset_tools.file_tools,       # æ–‡ä»¶æ“ä½œ
    preset_tools.http_tools,       # HTTPè¯·æ±‚
    oxy.ReActAgent(
        name="multi_skill_agent",
        tools=[
            "document_tools",  # æ–‡æ¡£å¤„ç†
            "file_tools",      # æ–‡ä»¶æ“ä½œ
            "http_tools"       # ç½‘ç»œè¯·æ±‚
        ],
    ),
]
```

---

## å·¥å…·APIè¯¦è§£

### PDFå¤„ç†å·¥å…·

#### 1. extract_pdf_text - æå–PDFæ–‡æœ¬

```python
extract_pdf_text(
    path: str,                      # PDFæ–‡ä»¶è·¯å¾„
    page_range: Optional[str] = None,  # é¡µç èŒƒå›´ï¼š"1-5" æˆ– "1,3,5"
    max_chars_per_page: int = 10000    # å•é¡µæœ€å¤§å­—ç¬¦æ•°
) -> str  # è¿”å›JSONå­—ç¬¦ä¸²
```

**è¿”å›ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "file_path": "document.pdf",
  "total_pages": 10,
  "extracted_pages": 5,
  "pages": [
    {
      "page_number": 1,
      "text": "é¡µé¢æ–‡æœ¬å†…å®¹...",
      "char_count": 1523,
      "has_images": true
    }
  ]
}
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- æ–‡æ¡£å†…å®¹æå–ä¸åˆ†æ
- RAGç³»ç»Ÿçš„æ–‡æ¡£é¢„å¤„ç†
- æ–‡æœ¬æœç´¢ä¸ç´¢å¼•æ„å»º

#### 2. extract_pdf_tables - æå–PDFè¡¨æ ¼

```python
extract_pdf_tables(
    path: str,
    page_range: Optional[str] = None,
    table_settings: Optional[Dict] = None  # è¡¨æ ¼è¯†åˆ«å‚æ•°
) -> str
```

**è¿”å›ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "table_count": 2,
  "tables": [
    {
      "page": 1,
      "table_index": 1,
      "headers": ["å§“å", "å¹´é¾„", "åŸå¸‚"],
      "rows": [
        ["å¼ ä¸‰", "30", "åŒ—äº¬"],
        ["æå››", "25", "ä¸Šæµ·"]
      ],
      "row_count": 2,
      "column_count": 3
    }
  ]
}
```

**é«˜çº§é…ç½®ï¼š**
```python
# è‡ªå®šä¹‰è¡¨æ ¼è¯†åˆ«ç­–ç•¥
table_settings = {
    "vertical_strategy": "lines",    # å‚ç›´çº¿è¯†åˆ«ç­–ç•¥
    "horizontal_strategy": "lines",  # æ°´å¹³çº¿è¯†åˆ«ç­–ç•¥
    "snap_tolerance": 3,             # çº¿æ¡å¯¹é½å®¹å·®
    "join_tolerance": 3              # è¡¨æ ¼åˆå¹¶å®¹å·®
}

result = extract_pdf_tables(
    "complex_table.pdf",
    table_settings=table_settings
)
```

#### 3. extract_pdf_images - æå–PDFå›¾åƒ

```python
extract_pdf_images(
    path: str,
    output_dir: str,             # å›¾åƒä¿å­˜ç›®å½•
    page_range: Optional[str] = None,
    min_size: int = 1024        # æœ€å°å›¾åƒå¤§å°ï¼ˆå­—èŠ‚ï¼‰
) -> str
```

**è¿”å›ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "output_dir": "./images",
  "image_count": 5,
  "images": [
    {
      "page": 1,
      "filename": "image_p1_1.png",
      "path": "./images/image_p1_1.png",
      "size_bytes": 45678,
      "format": "png",
      "width": 800,
      "height": 600
    }
  ]
}
```

#### 4. merge_pdfs - åˆå¹¶PDF

```python
merge_pdfs(
    pdf_paths: List[str],           # PDFæ–‡ä»¶åˆ—è¡¨
    output_path: str,               # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    include_bookmarks: bool = True  # æ˜¯å¦ä¿ç•™ä¹¦ç­¾
) -> str
```

**ç¤ºä¾‹ï¼š**
```python
result = merge_pdfs(
    pdf_paths=[
        "chapter1.pdf",
        "chapter2.pdf",
        "chapter3.pdf"
    ],
    output_path="full_book.pdf"
)
```

#### 5. split_pdf - æ‹†åˆ†PDF

```python
split_pdf(
    path: str,
    split_ranges: List[str],  # æ‹†åˆ†èŒƒå›´åˆ—è¡¨
    output_dir: str,
    name_prefix: str = "split"
) -> str
```

**ç¤ºä¾‹ï¼š**
```python
# å°†100é¡µPDFæ‹†åˆ†ä¸º3ä¸ªæ–‡ä»¶
result = split_pdf(
    path="large_document.pdf",
    split_ranges=["1-30", "31-70", "71-100"],
    output_dir="./chapters"
)
```

#### 6. get_pdf_info - è·å–PDFå…ƒæ•°æ®

```python
get_pdf_info(path: str) -> str
```

**è¿”å›ä¿¡æ¯ï¼š**
- æ–‡æ¡£å±æ€§ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€ä¸»é¢˜ã€å…³é”®è¯ï¼‰
- é¡µé¢ä¿¡æ¯ï¼ˆæ€»é¡µæ•°ã€é¡µé¢å°ºå¯¸ï¼‰
- æŠ€æœ¯ä¿¡æ¯ï¼ˆPDFç‰ˆæœ¬ã€åŠ å¯†çŠ¶æ€ï¼‰
- å†…å®¹ç»Ÿè®¡ï¼ˆæ–‡æœ¬é‡ã€å›¾åƒæ•°é‡ï¼‰

### Wordæ–‡æ¡£å·¥å…·

#### 7. read_docx - è¯»å–Wordæ–‡æ¡£

```python
read_docx(
    path: str,
    include_tables: bool = True,
    max_paragraphs: int = 1000
) -> str
```

**è¿”å›ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "statistics": {
    "paragraph_count": 25,
    "table_count": 2,
    "word_count": 1523,
    "char_count": 8965
  },
  "paragraphs": [
    {
      "index": 1,
      "text": "æ®µè½å†…å®¹...",
      "style": "Heading 1"
    }
  ],
  "tables": [
    {
      "table_index": 1,
      "row_count": 5,
      "column_count": 3,
      "headers": ["åˆ—1", "åˆ—2", "åˆ—3"],
      "rows": [["æ•°æ®1", "æ•°æ®2", "æ•°æ®3"]]
    }
  ]
}
```

#### 8. extract_docx_text - æå–Wordçº¯æ–‡æœ¬

```python
extract_docx_text(path: str) -> str
```

å¿«é€Ÿæå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…å«æ ¼å¼ä¿¡æ¯ã€‚

### Excelæ–‡æ¡£å·¥å…·

#### 9. read_excel - è¯»å–Excel

```python
read_excel(
    path: str,
    sheet_name: Optional[str] = None,  # å·¥ä½œè¡¨åç§°
    max_rows: int = 100,               # æœ€å¤§è¡Œæ•°
    has_header: bool = True            # æ˜¯å¦æœ‰è¡¨å¤´
) -> str
```

**è¿”å›ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "sheet_name": "é”€å”®æ•°æ®",
  "available_sheets": ["é”€å”®æ•°æ®", "ç»Ÿè®¡æŠ¥è¡¨"],
  "statistics": {
    "row_count": 50,
    "column_count": 5,
    "has_header": true
  },
  "headers": ["æ—¥æœŸ", "äº§å“", "æ•°é‡", "å•ä»·", "æ€»é¢"],
  "rows": [
    ["2024-01-01", "äº§å“A", 10, 99.9, 999],
    ["2024-01-02", "äº§å“B", 20, 49.9, 998]
  ]
}
```

#### 10. list_excel_sheets - åˆ—å‡ºå·¥ä½œè¡¨

```python
list_excel_sheets(path: str) -> str
```

è¿”å›æ‰€æœ‰å·¥ä½œè¡¨çš„åç§°å’ŒåŸºæœ¬ä¿¡æ¯ã€‚

### é€šç”¨å·¥å…·

#### 11. detect_document_format - æ£€æµ‹æ–‡æ¡£æ ¼å¼

```python
detect_document_format(path: str) -> str
```

**è¿”å›ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "filename": "report.pdf",
  "extension": ".pdf",
  "size_mb": 2.5,
  "format_info": {
    "type": "PDF",
    "category": "Portable Document Format",
    "supported": true,
    "tools": [
      "extract_pdf_text",
      "extract_pdf_tables",
      "extract_pdf_images"
    ]
  }
}
```

---

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼ˆRAGï¼‰

```python
from oxygent.preset_tools.document_tools import extract_pdf_text
from oxygent.databases.db_vector import VectorDB
import json

# 1. æå–PDFå†…å®¹
result = extract_pdf_text("knowledge_base.pdf")
data = json.loads(result)

# 2. åˆ†å—å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
vector_db = VectorDB()
for page in data['pages']:
    chunks = split_text(page['text'], chunk_size=500)
    for chunk in chunks:
        embedding = get_embedding(chunk)
        vector_db.add(
            text=chunk,
            embedding=embedding,
            metadata={
                "page": page['page_number'],
                "source": "knowledge_base.pdf"
            }
        )

# 3. ä½¿ç”¨Agentå›ç­”é—®é¢˜
query = "æ–‡æ¡£ä¸­å…³äºXXçš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
relevant_chunks = vector_db.search(query, top_k=5)
answer = agent.ask(query, context=relevant_chunks)
```

### åœºæ™¯2ï¼šæ‰¹é‡æŠ¥è¡¨å¤„ç†

```python
import json
from pathlib import Path
from oxygent.preset_tools.document_tools import (
    read_excel,
    extract_pdf_tables
)

reports_dir = Path("./monthly_reports")
all_data = []

# å¤„ç†ExcelæŠ¥è¡¨
for excel_file in reports_dir.glob("*.xlsx"):
    result = read_excel(str(excel_file))
    data = json.loads(result)
    if data['success']:
        all_data.extend(data['rows'])

# å¤„ç†PDFæŠ¥è¡¨
for pdf_file in reports_dir.glob("*.pdf"):
    result = extract_pdf_tables(str(pdf_file))
    data = json.loads(result)
    if data['success']:
        for table in data['tables']:
            all_data.extend(table['rows'])

# ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
generate_summary_report(all_data)
```

### åœºæ™¯3ï¼šæ–‡æ¡£å†…å®¹å®¡è®¡

```python
from oxygent.preset_tools.document_tools import (
    extract_pdf_text,
    get_pdf_info
)

def audit_document(pdf_path):
    """å®¡è®¡æ–‡æ¡£å†…å®¹ï¼Œæ£€æµ‹æ•æ„Ÿä¿¡æ¯"""
    # è·å–æ–‡æ¡£ä¿¡æ¯
    info = json.loads(get_pdf_info(pdf_path))
    
    # æå–å…¨æ–‡
    text_result = json.loads(extract_pdf_text(pdf_path))
    
    # æ•æ„Ÿè¯æ£€æµ‹
    sensitive_words = ["æœºå¯†", "ä¿å¯†", "å†…éƒ¨"]
    found_sensitive = []
    
    for page in text_result['pages']:
        for word in sensitive_words:
            if word in page['text']:
                found_sensitive.append({
                    "page": page['page_number'],
                    "keyword": word
                })
    
    # ç”Ÿæˆå®¡è®¡æŠ¥å‘Š
    return {
        "file": pdf_path,
        "pages": info['document_properties']['page_count'],
        "author": info['document_metadata']['author'],
        "sensitive_items": found_sensitive,
        "risk_level": "high" if found_sensitive else "low"
    }
```

### åœºæ™¯4ï¼šå¤šè¯­è¨€æ–‡æ¡£ç¿»è¯‘

```python
async def translate_document(pdf_path, target_lang="en"):
    """æå–æ–‡æ¡£å†…å®¹å¹¶ç¿»è¯‘"""
    from oxygent import MAS, oxy, preset_tools
    
    # æå–åŸæ–‡
    result = extract_pdf_text(pdf_path)
    data = json.loads(result)
    
    # ä½¿ç”¨Agentç¿»è¯‘
    oxy_space = [
        your_llm,
        preset_tools.document_tools,
        oxy.ReActAgent(
            name="translator_agent",
            desc=f"å°†æ–‡æœ¬ç¿»è¯‘ä¸º{target_lang}",
            tools=["document_tools"]
        )
    ]
    
    translations = []
    async with MAS(oxy_space=oxy_space) as mas:
        for page in data['pages']:
            query = f"å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸º{target_lang}ï¼š{page['text'][:500]}"
            response = await mas.ask("translator_agent", query)
            translations.append(response)
    
    return translations
```

---

## æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
- CPU: Intel i7-12700K
- RAM: 32GB
- æ–‡ä»¶: 100é¡µPDF, 10MB

### æ€§èƒ½å¯¹æ¯”

| æ“ä½œ | PyMuPDF | PyPDF2 | pdfplumber | æå‡æ¯”ä¾‹ |
|------|---------|--------|------------|----------|
| æ–‡æœ¬æå– | 0.3s | 1.2s | 2.5s | **4-8x** |
| è¡¨æ ¼æå– | 1.5s | N/A | 3.8s | **2.5x** |
| å›¾åƒæå– | 0.8s | 2.1s | N/A | **2.6x** |
| PDFåˆå¹¶ | 0.5s | 1.8s | N/A | **3.6x** |
| å†…å­˜å ç”¨ | 80MB | 150MB | 200MB | **èŠ‚çœ50%** |

### å¤§æ–‡ä»¶å¤„ç†èƒ½åŠ›

| æ–‡ä»¶å¤§å° | é¡µæ•° | å¤„ç†æ—¶é—´ | å³°å€¼å†…å­˜ |
|---------|------|---------|----------|
| 1MB | 10é¡µ | 0.1s | 50MB |
| 10MB | 100é¡µ | 0.8s | 120MB |
| 50MB | 500é¡µ | 3.5s | 280MB |
| 100MB | 1000é¡µ | 7.2s | 450MB |

---



## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
import json
from oxygent.preset_tools.document_tools import extract_pdf_text

def safe_extract_pdf(pdf_path):
    """å®‰å…¨çš„PDFæå–ï¼Œå¸¦å®Œæ•´é”™è¯¯å¤„ç†"""
    try:
        result_str = extract_pdf_text(pdf_path)
        result = json.loads(result_str)
        
        if result.get('success'):
            return result
        else:
            logger.error(f"æå–å¤±è´¥: {result.get('error')}")
            return None
            
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æé”™è¯¯: {e}")
        return None
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
        return None
```

### 2. æ€§èƒ½ä¼˜åŒ–

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def process_documents_parallel(file_list):
    """å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(
                executor,
                extract_pdf_text,
                file_path
            )
            for file_path in file_list
        ]
        results = await asyncio.gather(*tasks)
    
    return results
```

### 3. ç»“æœç¼“å­˜

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def extract_pdf_cached(pdf_path, page_range):
    """ç¼“å­˜PDFæå–ç»“æœ"""
    return extract_pdf_text(pdf_path, page_range)

# æˆ–ä½¿ç”¨æ–‡ä»¶å†…å®¹å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
def get_file_hash(file_path):
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

cache = {}

def extract_with_content_cache(pdf_path):
    file_hash = get_file_hash(pdf_path)
    
    if file_hash in cache:
        return cache[file_hash]
    
    result = extract_pdf_text(pdf_path)
    cache[file_hash] = result
    return result
```




